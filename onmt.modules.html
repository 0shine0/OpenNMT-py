

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>OpenNMT Modules &mdash; OpenNMT-py  documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="OpenNMT-py  documentation" href="index.html"/>
        <link rel="next" title="Contributors" href="CONTRIBUTORS.html"/>
        <link rel="prev" title="OpenNMT Data" href="onmt.io.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> OpenNMT-py
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="main.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="extended.html">Example: Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Summarization.html">Example: Summarization</a></li>
<li class="toctree-l1"><a class="reference internal" href="im2text.html">Example: Image to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech2text.html">Example: Speech to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/preprocess.html">Options: preprocess.py:</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/train.html">Options: train.py:</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/translate.html">Options: translate.py:</a></li>
<li class="toctree-l1"><a class="reference internal" href="onmt.html">OpenNMT Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="onmt.io.html">OpenNMT Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">OpenNMT Modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#core-modules">Core Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#architecture-transfomer">Architecture: Transfomer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Architecture: Transfomer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.SRU">Architecture: SRU</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alternative-encoders">Alternative Encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="#extensionsk">ExtensionsK</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alternative-attention-multi-headed">Alternative Attention: Multi Headed</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alternative-attention-structured-attention">Alternative Attention: Structured Attention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTORS.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="ref.html">References</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OpenNMT-py</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>OpenNMT Modules</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/onmt.modules.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="opennmt-modules">
<h1>OpenNMT Modules<a class="headerlink" href="#opennmt-modules" title="Permalink to this headline">¶</a></h1>
<div class="section" id="core-modules">
<h2>Core Modules<a class="headerlink" href="#core-modules" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.Embeddings">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">Embeddings</code><span class="sig-paren">(</span><em>word_vec_size</em>, <em>position_encoding</em>, <em>feat_merge</em>, <em>feat_vec_exponent</em>, <em>feat_vec_size</em>, <em>dropout</em>, <em>word_padding_idx</em>, <em>feat_padding_idx</em>, <em>word_vocab_size</em>, <em>feat_vocab_sizes=[]</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Words embeddings dictionary for encoder/decoder.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>word_vec_size</strong> (<em>int</em>) – size of the dictionary of embeddings.</li>
<li><strong>position_encoding</strong> (<em>bool</em>) – use a sin to mark relative words positions.</li>
<li><strong>feat_merge</strong> (<em>string</em>) – merge action for the features embeddings:
concat, sum or mlp.</li>
<li><strong>feat_vec_exponent</strong> (<em>float</em>) – when using ‘-feat_merge concat’, feature
embedding size is N^feat_dim_exponent, where N is the
number of values of feature takes.</li>
<li><strong>feat_vec_size</strong> (<em>int</em>) – embedding dimension for features when using
‘-feat_merge mlp’</li>
<li><strong>dropout</strong> (<em>float</em>) – dropout probability.</li>
<li><strong>word_padding_idx</strong> (<em>int</em>) – padding index for words in the embeddings.</li>
<li><strong>feats_padding_idx</strong> (<em>[</em><em>int</em><em>]</em>) – padding index for a list of features
in the embeddings.</li>
<li><strong>word_vocab_size</strong> (<em>int</em>) – size of dictionary of embeddings for words.</li>
<li><strong>feat_vocab_sizes</strong> (<em>[</em><em>int</em><em>]</em><em>, </em><em>optional</em>) – list of size of dictionary
of embeddings for each feature.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="onmt.modules.Embeddings.emb_luts">
<code class="descname">emb_luts</code><a class="headerlink" href="#onmt.modules.Embeddings.emb_luts" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.Embeddings.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Embeddings.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the embeddings for words, and features if there are any.
:param input: len x batch x nfeat
:type input: LongTensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">len x batch x self.embedding_size</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">emb (FloatTensor)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.Embeddings.load_pretrained_vectors">
<code class="descname">load_pretrained_vectors</code><span class="sig-paren">(</span><em>emb_file</em>, <em>fixed</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Embeddings.load_pretrained_vectors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="onmt.modules.Embeddings.word_lut">
<code class="descname">word_lut</code><a class="headerlink" href="#onmt.modules.Embeddings.word_lut" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.StackedGRU">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">StackedGRU</code><span class="sig-paren">(</span><em>num_layers</em>, <em>input_size</em>, <em>rnn_size</em>, <em>dropout</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.StackedGRU" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="onmt.modules.StackedGRU.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>hidden</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.StackedGRU.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.StackedLSTM">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">StackedLSTM</code><span class="sig-paren">(</span><em>num_layers</em>, <em>input_size</em>, <em>rnn_size</em>, <em>dropout</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.StackedLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Our own implementation of stacked LSTM.
Needed for the decoder, because we do input feeding.</p>
<dl class="method">
<dt id="onmt.modules.StackedLSTM.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>hidden</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.StackedLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.GlobalAttention">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">GlobalAttention</code><span class="sig-paren">(</span><em>dim</em>, <em>coverage=False</em>, <em>attn_type='dot'</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.GlobalAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Luong Attention.</p>
<p>Global attention takes a matrix and a query vector. It
then computes a parameterized convex combination of the matrix
based on the input query.</p>
<blockquote>
<div><dl class="docutils">
<dt>H_1 H_2 H_3 … H_n</dt>
<dd><dl class="first last docutils">
<dt>q   q   q       q</dt>
<dd><div class="first last line-block">
<div class="line">|   |       |
|   |      /
        …..
      |  /
        a</div>
</div>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>Constructs a unit mapping.
$$(H_1 + H_n, q) =&gt; (a)$$
Where H is of <cite>batch x n x dim</cite> and q is of <cite>batch x dim</cite>.</p>
<p>Luong Attention (dot, general):
The full function is
$$  anh(W_2 [(softmax((W_1 q + b_1) H) H), q] + b_2)$$.</p>
<ul class="simple">
<li>dot: $$score(h_t,{overline{h}}_s) = h_t^T{overline{h}}_s$$</li>
<li>general: $$score(h_t,{overline{h}}_s) = h_t^T W_a {overline{h}}_s$$</li>
</ul>
<p>Bahdanau Attention (mlp):
$$c = sum_{j=1}^{SeqLength}_jh_j$$.
The Alignment-function $$a$$ computes an alignment as:
$$a_j = softmax(v_a^T       anh(W_a q + U_a h_j) )$$.</p>
<dl class="method">
<dt id="onmt.modules.GlobalAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>context</em>, <em>context_lengths=None</em>, <em>coverage=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.GlobalAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>input (FloatTensor): batch x tgt_len x dim: decoder’s rnn’s output.
context (FloatTensor): batch x src_len x dim: src hidden states
context_lengths (LongTensor): the source context lengths.
coverage (FloatTensor): None (not supported yet)</p>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.GlobalAttention.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>h_t</em>, <em>h_s</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.GlobalAttention.score" title="Permalink to this definition">¶</a></dt>
<dd><p>h_t (FloatTensor): batch x tgt_len x dim
h_s (FloatTensor): batch x src_len x dim
returns scores (FloatTensor): batch x tgt_len x src_len:</p>
<blockquote>
<div>raw attention scores for each src index</div></blockquote>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="architecture-transfomer">
<h2>Architecture: Transfomer<a class="headerlink" href="#architecture-transfomer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.TransformerEncoder">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">TransformerEncoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>hidden_size</em>, <em>dropout</em>, <em>embeddings</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.TransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">onmt.Models.EncoderBase</span></code></p>
<p>The Transformer encoder from “Attention is All You Need”.</p>
<dl class="method">
<dt id="onmt.modules.TransformerEncoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>lengths=None</em>, <em>hidden=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.TransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>See EncoderBase.forward() for description of args and returns.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.TransformerDecoder">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">TransformerDecoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>hidden_size</em>, <em>attn_type</em>, <em>copy_attn</em>, <em>dropout</em>, <em>embeddings</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.TransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The Transformer decoder from “Attention is All You Need”.</p>
<dl class="method">
<dt id="onmt.modules.TransformerDecoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>context</em>, <em>state</em>, <em>context_lengths=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.TransformerDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward through the TransformerDecoder.
:param input: a sequence of input tokens tensors</p>
<blockquote>
<div>of size (len x batch x nfeats).</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>context</strong> (<em>FloatTensor</em>) – output(tensor sequence) from the encoder
of size (src_len x batch x hidden_size).</li>
<li><strong>state</strong> (<em>FloatTensor</em>) – hidden state from the encoder RNN for
initializing the decoder.</li>
<li><strong>context_lengths</strong> (<em>LongTensor</em>) – the source context lengths, this is
not used for TransformerDecoder, but
for interface compatibility.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>a Tensor sequence of output from the decoder</dt>
<dd><p class="first last">of shape (len x batch x hidden_size).</p>
</dd>
</dl>
<p>state (FloatTensor): final hidden state from the decoder.
attns (dict of (str, FloatTensor)): a dictionary of different</p>
<blockquote>
<div><p>type of attention Tensor from the decoder
of shape (src_len x batch).</p>
</div></blockquote>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">outputs (FloatTensor)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="id1">
<h2>Architecture: Transfomer<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-onmt.modules.Conv2Conv"></span><p>Implementation of “Convolutional Sequence to Sequence Learning”</p>
<dl class="class">
<dt id="onmt.modules.Conv2Conv.CNNDecoder">
<em class="property">class </em><code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">CNNDecoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>hidden_size</em>, <em>attn_type</em>, <em>copy_attn</em>, <em>cnn_kernel_width</em>, <em>dropout</em>, <em>embeddings</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Decoder built on CNN, which consists of resduial convolutional layers,
with ConvMultiStepAttention.</p>
<dl class="method">
<dt id="onmt.modules.Conv2Conv.CNNDecoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>context</em>, <em>state</em>, <em>context_lengths=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward through the CNNDecoder.
:param input: a sequence of input tokens tensors</p>
<blockquote>
<div>of size (len x batch x nfeats).</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>context</strong> (<em>FloatTensor</em>) – output(tensor sequence) from the encoder
CNN of size (src_len x batch x hidden_size).</li>
<li><strong>state</strong> (<em>FloatTensor</em>) – hidden state from the encoder CNN for
initializing the decoder.</li>
<li><strong>context_lengths</strong> (<em>LongTensor</em>) – the source context lengths, this is
not used for CNNDecoder, but for interface compatibility.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>a Tensor sequence of output from the decoder</dt>
<dd><p class="first last">of shape (len x batch x hidden_size).</p>
</dd>
</dl>
<p>state (FloatTensor): final hidden state from the decoder.
attns (dict of (str, FloatTensor)): a dictionary of different</p>
<blockquote>
<div><p>type of attention Tensor from the decoder
of shape (src_len x batch).</p>
</div></blockquote>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">outputs (FloatTensor)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.Conv2Conv.CNNDecoder.init_decoder_state">
<code class="descname">init_decoder_state</code><span class="sig-paren">(</span><em>src</em>, <em>context</em>, <em>enc_hidden</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoder.init_decoder_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Conv2Conv.CNNDecoderState">
<em class="property">class </em><code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">CNNDecoderState</code><span class="sig-paren">(</span><em>context</em>, <em>enc_hidden</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoderState" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="onmt.modules.Conv2Conv.CNNDecoderState.repeat_beam_size_times">
<code class="descname">repeat_beam_size_times</code><span class="sig-paren">(</span><em>beam_size</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoderState.repeat_beam_size_times" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeat beam_size times along batch dimension.</p>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.Conv2Conv.CNNDecoderState.update_state">
<code class="descname">update_state</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoderState.update_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Called for every decoder forward pass.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Conv2Conv.CNNEncoder">
<em class="property">class </em><code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">CNNEncoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>hidden_size</em>, <em>cnn_kernel_width</em>, <em>dropout</em>, <em>embeddings</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Encoder built on CNN.</p>
<dl class="method">
<dt id="onmt.modules.Conv2Conv.CNNEncoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>lengths=None</em>, <em>hidden=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>See EncoderBase.forward() for description of args and returns.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Conv2Conv.GatedConv">
<em class="property">class </em><code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">GatedConv</code><span class="sig-paren">(</span><em>input_size</em>, <em>width=3</em>, <em>dropout=0.2</em>, <em>nopad=False</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.GatedConv" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="onmt.modules.Conv2Conv.GatedConv.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x_var</em>, <em>hidden=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.GatedConv.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Conv2Conv.StackedCNN">
<em class="property">class </em><code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">StackedCNN</code><span class="sig-paren">(</span><em>num_layers</em>, <em>input_size</em>, <em>cnn_kernel_width=3</em>, <em>dropout=0.2</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.StackedCNN" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="onmt.modules.Conv2Conv.StackedCNN.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>hidden=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.StackedCNN.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="onmt.modules.Conv2Conv.shape_transform">
<code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">shape_transform</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.Conv2Conv.shape_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Tranform the size of the tensors to fit for conv input.</p>
</dd></dl>

</div>
<div class="section" id="module-onmt.modules.SRU">
<span id="architecture-sru"></span><h2>Architecture: SRU<a class="headerlink" href="#module-onmt.modules.SRU" title="Permalink to this headline">¶</a></h2>
<p>Implementation of “Training RNNs as Fast as CNNs”.
TODO: turn to pytorch’s implementation when it is available.</p>
<p>This implementation is adpoted from the author of the paper:
<a class="reference external" href="https://github.com/taolei87/sru/blob/master/cuda_functional.py">https://github.com/taolei87/sru/blob/master/cuda_functional.py</a>.</p>
<dl class="class">
<dt id="onmt.modules.SRU.CheckSRU">
<em class="property">class </em><code class="descclassname">onmt.modules.SRU.</code><code class="descname">CheckSRU</code><span class="sig-paren">(</span><em>option_strings</em>, <em>dest</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.CheckSRU" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="onmt.modules.SRU.SRU">
<em class="property">class </em><code class="descclassname">onmt.modules.SRU.</code><code class="descname">SRU</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>num_layers=2</em>, <em>dropout=0</em>, <em>rnn_dropout=0</em>, <em>bidirectional=False</em>, <em>use_tanh=1</em>, <em>use_relu=0</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRU" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="onmt.modules.SRU.SRU.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>c0=None</em>, <em>return_hidden=True</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRU.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.SRU.SRU.set_bias">
<code class="descname">set_bias</code><span class="sig-paren">(</span><em>bias_val=0</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRU.set_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.SRU.SRUCell">
<em class="property">class </em><code class="descclassname">onmt.modules.SRU.</code><code class="descname">SRUCell</code><span class="sig-paren">(</span><em>n_in</em>, <em>n_out</em>, <em>dropout=0</em>, <em>rnn_dropout=0</em>, <em>bidirectional=False</em>, <em>use_tanh=1</em>, <em>use_relu=0</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRUCell" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="onmt.modules.SRU.SRUCell.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>c0=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRUCell.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.SRU.SRUCell.get_dropout_mask_">
<code class="descname">get_dropout_mask_</code><span class="sig-paren">(</span><em>size</em>, <em>p</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRUCell.get_dropout_mask_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.SRU.SRUCell.init_weight">
<code class="descname">init_weight</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRUCell.init_weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.SRU.SRUCell.set_bias">
<code class="descname">set_bias</code><span class="sig-paren">(</span><em>bias_val=0</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRUCell.set_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.SRU.SRU_Compute">
<em class="property">class </em><code class="descclassname">onmt.modules.SRU.</code><code class="descname">SRU_Compute</code><span class="sig-paren">(</span><em>activation_type</em>, <em>d_out</em>, <em>bidirectional=False</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRU_Compute" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="onmt.modules.SRU.SRU_Compute.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>grad_h</em>, <em>grad_last</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRU_Compute.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.SRU.SRU_Compute.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>u</em>, <em>x</em>, <em>bias</em>, <em>init=None</em>, <em>mask_h=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.SRU_Compute.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="onmt.modules.SRU.check_sru_requirement">
<code class="descclassname">onmt.modules.SRU.</code><code class="descname">check_sru_requirement</code><span class="sig-paren">(</span><em>abort=False</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.SRU.check_sru_requirement" title="Permalink to this definition">¶</a></dt>
<dd><p>Return True if check pass; if check fails and abort is True,
raise an Exception, othereise return False.</p>
</dd></dl>

</div>
<div class="section" id="alternative-encoders">
<h2>Alternative Encoders<a class="headerlink" href="#alternative-encoders" title="Permalink to this headline">¶</a></h2>
<p>onmt.modules.AudioEncoder</p>
<dl class="class">
<dt id="onmt.modules.AudioEncoder">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">AudioEncoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>bidirectional</em>, <em>rnn_size</em>, <em>dropout</em>, <em>sample_rate</em>, <em>window_size</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.AudioEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Encoder recurrent neural network for Images.</p>
<dl class="method">
<dt id="onmt.modules.AudioEncoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>lengths=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.AudioEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.AudioEncoder.load_pretrained_vectors">
<code class="descname">load_pretrained_vectors</code><span class="sig-paren">(</span><em>opt</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.AudioEncoder.load_pretrained_vectors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<p>onmt.modules.ImageEncoder</p>
<dl class="class">
<dt id="onmt.modules.ImageEncoder">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">ImageEncoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>bidirectional</em>, <em>rnn_size</em>, <em>dropout</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.ImageEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Encoder recurrent neural network for Images.</p>
<dl class="method">
<dt id="onmt.modules.ImageEncoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>lengths=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.ImageEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.ImageEncoder.load_pretrained_vectors">
<code class="descname">load_pretrained_vectors</code><span class="sig-paren">(</span><em>opt</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.ImageEncoder.load_pretrained_vectors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="extensionsk">
<h2>ExtensionsK<a class="headerlink" href="#extensionsk" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.CopyGenerator">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">CopyGenerator</code><span class="sig-paren">(</span><em>opt</em>, <em>src_dict</em>, <em>tgt_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.CopyGenerator" title="Permalink to this definition">¶</a></dt>
<dd><p>Generator module that additionally considers copying
words directly from the source.</p>
<dl class="method">
<dt id="onmt.modules.CopyGenerator.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>hidden</em>, <em>attn</em>, <em>src_map</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.CopyGenerator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes p(w) = p(z=1) p_{copy}(w|z=0)  +  p(z=0) * p_{softmax}(w|z=0)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.CopyGeneratorLossCompute">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">CopyGeneratorLossCompute</code><span class="sig-paren">(</span><em>generator</em>, <em>tgt_vocab</em>, <em>dataset</em>, <em>force_copy</em>, <em>eps=1e-20</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.CopyGeneratorLossCompute" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy Generator Loss Computation.</p>
<dl class="method">
<dt id="onmt.modules.CopyGeneratorLossCompute.compute_loss">
<code class="descname">compute_loss</code><span class="sig-paren">(</span><em>batch</em>, <em>output</em>, <em>target</em>, <em>copy_attn</em>, <em>align</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.CopyGeneratorLossCompute.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss. The args must match self.make_shard_state().
:param batch: the current batch.
:param output: the predict output from the model.
:param target: the validate target to compare output with.
:param copy_attn: the copy attention value.
:param align: the align info.</p>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.CopyGeneratorLossCompute.make_shard_state">
<code class="descname">make_shard_state</code><span class="sig-paren">(</span><em>batch</em>, <em>output</em>, <em>range_</em>, <em>attns</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.CopyGeneratorLossCompute.make_shard_state" title="Permalink to this definition">¶</a></dt>
<dd><p>See base class for args description.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="alternative-attention-multi-headed">
<h2>Alternative Attention: Multi Headed<a class="headerlink" href="#alternative-attention-multi-headed" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.MultiHeadedAttention">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">MultiHeadedAttention</code><span class="sig-paren">(</span><em>head_count</em>, <em>model_dim</em>, <em>p=0.1</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.MultiHeadedAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-Head Attention module from
“Attention is All You Need”.</p>
<dl class="method">
<dt id="onmt.modules.MultiHeadedAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>key</em>, <em>value</em>, <em>query</em>, <em>mask=None</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.MultiHeadedAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="alternative-attention-structured-attention">
<h2>Alternative Attention: Structured Attention<a class="headerlink" href="#alternative-attention-structured-attention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.MatrixTree">
<em class="property">class </em><code class="descclassname">onmt.modules.</code><code class="descname">MatrixTree</code><span class="sig-paren">(</span><em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.MatrixTree" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of the matrix-tree theorem for computing marginals
of non-projective dependency parsing. This attention layer is used
in the paper “Learning Structured Text Representations.”</p>
<p><a class="reference internal" href="ref.html#dblp-journals-corr-liul17d" id="id2">[LL17]</a></p>
<dl class="method">
<dt id="onmt.modules.MatrixTree.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.MatrixTree.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<span class="target" id="module-onmt.modules.WeightNorm"></span><p>Implementation of “Weight Normalization: A Simple Reparameterization
to Accelerate Training of Deep Neural Networks”
As a reparameterization method, weight normalization is same
as BatchNormalization, but it doesn’t depend on minibatch.</p>
<dl class="class">
<dt id="onmt.modules.WeightNorm.WeightNormConv2d">
<em class="property">class </em><code class="descclassname">onmt.modules.WeightNorm.</code><code class="descname">WeightNormConv2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>init_scale=1.0</em>, <em>polyak_decay=0.9995</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.conv.Conv2d</span></code></p>
<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormConv2d.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>init=False</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConv2d.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormConv2d.reset_parameters">
<code class="descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConv2d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.WeightNorm.WeightNormConvTranspose2d">
<em class="property">class </em><code class="descclassname">onmt.modules.WeightNorm.</code><code class="descname">WeightNormConvTranspose2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>init_scale=1.0</em>, <em>polyak_decay=0.9995</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.conv.ConvTranspose2d</span></code></p>
<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormConvTranspose2d.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>init=False</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConvTranspose2d.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormConvTranspose2d.reset_parameters">
<code class="descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConvTranspose2d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.WeightNorm.WeightNormLinear">
<em class="property">class </em><code class="descclassname">onmt.modules.WeightNorm.</code><code class="descname">WeightNormLinear</code><span class="sig-paren">(</span><em>in_features</em>, <em>out_features</em>, <em>init_scale=1.0</em>, <em>polyak_decay=0.9995</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormLinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.linear.Linear</span></code></p>
<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormLinear.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>init=False</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormLinear.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormLinear.reset_parameters">
<code class="descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormLinear.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="onmt.modules.WeightNorm.get_var_maybe_avg">
<code class="descclassname">onmt.modules.WeightNorm.</code><code class="descname">get_var_maybe_avg</code><span class="sig-paren">(</span><em>namespace</em>, <em>var_name</em>, <em>training</em>, <em>polyak_decay</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.get_var_maybe_avg" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="onmt.modules.WeightNorm.get_vars_maybe_avg">
<code class="descclassname">onmt.modules.WeightNorm.</code><code class="descname">get_vars_maybe_avg</code><span class="sig-paren">(</span><em>namespace</em>, <em>var_names</em>, <em>training</em>, <em>polyak_decay</em><span class="sig-paren">)</span><a class="headerlink" href="#onmt.modules.WeightNorm.get_vars_maybe_avg" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="CONTRIBUTORS.html" class="btn btn-neutral float-right" title="Contributors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="onmt.io.html" class="btn btn-neutral" title="OpenNMT Data" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, srush.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>