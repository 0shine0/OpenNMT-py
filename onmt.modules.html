

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>onmt.modules package &mdash; OpenNMT-py  documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="OpenNMT-py  documentation" href="index.html"/>
        <link rel="up" title="onmt package" href="onmt.html"/>
        <link rel="next" title="Contributors" href="CONTRIBUTORS.html"/>
        <link rel="prev" title="onmt package" href="onmt.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> OpenNMT-py
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="main.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="extended.html">Example: Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Summarization.html">Example: Summarization</a></li>
<li class="toctree-l1"><a class="reference internal" href="im2text.html">Example: Image to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech2text.html">Example: Speech to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/preprocess.html">Options: preprocess.py:</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/train.html">Options: train.py:</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/translate.html">Options: translate.py:</a></li>
<li class="toctree-l1"><a class="reference internal" href="onmt.html">onmt package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">onmt.modules package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.AudioEncoder">onmt.modules.AudioEncoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.Conv2Conv">onmt.modules.Conv2Conv module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.ConvMultiStepAttention">onmt.modules.ConvMultiStepAttention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.CopyGenerator">onmt.modules.CopyGenerator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.Embeddings">onmt.modules.Embeddings module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.Gate">onmt.modules.Gate module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.GlobalAttention">onmt.modules.GlobalAttention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.ImageEncoder">onmt.modules.ImageEncoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.MultiHeadedAttn">onmt.modules.MultiHeadedAttn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.SRU">onmt.modules.SRU module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.StackedRNN">onmt.modules.StackedRNN module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.StructuredAttention">onmt.modules.StructuredAttention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.Transformer">onmt.modules.Transformer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.UtilClass">onmt.modules.UtilClass module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-onmt.modules.WeightNorm">onmt.modules.WeightNorm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-contents">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTORS.html">Contributors</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OpenNMT-py</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="onmt.html">onmt package</a> &raquo;</li>
        
      <li>onmt.modules package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/onmt.modules.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="onmt-modules-package">
<h1>onmt.modules package<a class="headerlink" href="#onmt-modules-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-onmt.modules.AudioEncoder">
<span id="onmt-modules-audioencoder-module"></span><h2>onmt.modules.AudioEncoder module<a class="headerlink" href="#module-onmt.modules.AudioEncoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.AudioEncoder.AudioEncoder">
<em class="property">class </em><code class="descclassname">onmt.modules.AudioEncoder.</code><code class="descname">AudioEncoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>bidirectional</em>, <em>rnn_size</em>, <em>dropout</em>, <em>sample_rate</em>, <em>window_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/AudioEncoder.html#AudioEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.AudioEncoder.AudioEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Encoder recurrent neural network for Images.</p>
<dl class="method">
<dt id="onmt.modules.AudioEncoder.AudioEncoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/AudioEncoder.html#AudioEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.AudioEncoder.AudioEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.AudioEncoder.AudioEncoder.load_pretrained_vectors">
<code class="descname">load_pretrained_vectors</code><span class="sig-paren">(</span><em>opt</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/AudioEncoder.html#AudioEncoder.load_pretrained_vectors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.AudioEncoder.AudioEncoder.load_pretrained_vectors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.Conv2Conv">
<span id="onmt-modules-conv2conv-module"></span><h2>onmt.modules.Conv2Conv module<a class="headerlink" href="#module-onmt.modules.Conv2Conv" title="Permalink to this headline">¶</a></h2>
<p>Implementation of “Convolutional Sequence to Sequence Learning”</p>
<dl class="class">
<dt id="onmt.modules.Conv2Conv.CNNDecoder">
<em class="property">class </em><code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">CNNDecoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>hidden_size</em>, <em>attn_type</em>, <em>copy_attn</em>, <em>cnn_kernel_width</em>, <em>dropout</em>, <em>embeddings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#CNNDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Decoder built on CNN, which consists of resduial convolutional layers,
with ConvMultiStepAttention.</p>
<dl class="method">
<dt id="onmt.modules.Conv2Conv.CNNDecoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>context</em>, <em>state</em>, <em>context_lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#CNNDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward through the CNNDecoder.
:param input: a sequence of input tokens tensors</p>
<blockquote>
<div>of size (len x batch x nfeats).</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>context</strong> (<em>FloatTensor</em>) – output(tensor sequence) from the encoder
CNN of size (src_len x batch x hidden_size).</li>
<li><strong>state</strong> (<em>FloatTensor</em>) – hidden state from the encoder CNN for
initializing the decoder.</li>
<li><strong>context_lengths</strong> (<em>LongTensor</em>) – the source context lengths, this is
not used for CNNDecoder, but for interface compatibility.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>a Tensor sequence of output from the decoder</dt>
<dd><p class="first last">of shape (len x batch x hidden_size).</p>
</dd>
</dl>
<p>state (FloatTensor): final hidden state from the decoder.
attns (dict of (str, FloatTensor)): a dictionary of different</p>
<blockquote>
<div><p>type of attention Tensor from the decoder
of shape (src_len x batch).</p>
</div></blockquote>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">outputs (FloatTensor)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.Conv2Conv.CNNDecoder.init_decoder_state">
<code class="descname">init_decoder_state</code><span class="sig-paren">(</span><em>src</em>, <em>context</em>, <em>enc_hidden</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#CNNDecoder.init_decoder_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoder.init_decoder_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Conv2Conv.CNNDecoderState">
<em class="property">class </em><code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">CNNDecoderState</code><span class="sig-paren">(</span><em>context</em>, <em>enc_hidden</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#CNNDecoderState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoderState" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="onmt.html#onmt.Models.DecoderState" title="onmt.Models.DecoderState"><code class="xref py py-class docutils literal"><span class="pre">onmt.Models.DecoderState</span></code></a></p>
<dl class="method">
<dt id="onmt.modules.Conv2Conv.CNNDecoderState.repeat_beam_size_times">
<code class="descname">repeat_beam_size_times</code><span class="sig-paren">(</span><em>beam_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#CNNDecoderState.repeat_beam_size_times"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoderState.repeat_beam_size_times" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeat beam_size times along batch dimension.</p>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.Conv2Conv.CNNDecoderState.update_state">
<code class="descname">update_state</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#CNNDecoderState.update_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNDecoderState.update_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Called for every decoder forward pass.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Conv2Conv.CNNEncoder">
<em class="property">class </em><code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">CNNEncoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>hidden_size</em>, <em>cnn_kernel_width</em>, <em>dropout</em>, <em>embeddings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#CNNEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="onmt.html#onmt.Models.EncoderBase" title="onmt.Models.EncoderBase"><code class="xref py py-class docutils literal"><span class="pre">onmt.Models.EncoderBase</span></code></a></p>
<p>Encoder built on CNN.</p>
<dl class="method">
<dt id="onmt.modules.Conv2Conv.CNNEncoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>lengths=None</em>, <em>hidden=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#CNNEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.CNNEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>See EncoderBase.forward() for description of args and returns.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Conv2Conv.GatedConv">
<em class="property">class </em><code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">GatedConv</code><span class="sig-paren">(</span><em>input_size</em>, <em>width=3</em>, <em>dropout=0.2</em>, <em>nopad=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#GatedConv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.GatedConv" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.Conv2Conv.GatedConv.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x_var</em>, <em>hidden=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#GatedConv.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.GatedConv.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Conv2Conv.StackedCNN">
<em class="property">class </em><code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">StackedCNN</code><span class="sig-paren">(</span><em>num_layers</em>, <em>input_size</em>, <em>cnn_kernel_width=3</em>, <em>dropout=0.2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#StackedCNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.StackedCNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.Conv2Conv.StackedCNN.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>hidden=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#StackedCNN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.StackedCNN.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="onmt.modules.Conv2Conv.shape_transform">
<code class="descclassname">onmt.modules.Conv2Conv.</code><code class="descname">shape_transform</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Conv2Conv.html#shape_transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Conv2Conv.shape_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Tranform the size of the tensors to fit for conv input.</p>
</dd></dl>

</div>
<div class="section" id="module-onmt.modules.ConvMultiStepAttention">
<span id="onmt-modules-convmultistepattention-module"></span><h2>onmt.modules.ConvMultiStepAttention module<a class="headerlink" href="#module-onmt.modules.ConvMultiStepAttention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.ConvMultiStepAttention.ConvMultiStepAttention">
<em class="property">class </em><code class="descclassname">onmt.modules.ConvMultiStepAttention.</code><code class="descname">ConvMultiStepAttention</code><span class="sig-paren">(</span><em>input_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/ConvMultiStepAttention.html#ConvMultiStepAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.ConvMultiStepAttention.ConvMultiStepAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.ConvMultiStepAttention.ConvMultiStepAttention.applyMask">
<code class="descname">applyMask</code><span class="sig-paren">(</span><em>mask</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/ConvMultiStepAttention.html#ConvMultiStepAttention.applyMask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.ConvMultiStepAttention.ConvMultiStepAttention.applyMask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.ConvMultiStepAttention.ConvMultiStepAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>base_target_emb</em>, <em>input</em>, <em>encoder_out_top</em>, <em>encoder_out_combine</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/ConvMultiStepAttention.html#ConvMultiStepAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.ConvMultiStepAttention.ConvMultiStepAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>It’s like Luong Attetion.
Conv attention takes a key matrix, a value matrix and a query vector.
Attention weight is calculated by key matrix with the query vector
and sum on the value matrix. And the same operation is applied
in each decode conv layer.
:param base_target_emb: target emb tensor
:param input: output of decode conv
:param encoder_out_t: the key matrix for calculation of attetion weight,</p>
<blockquote>
<div>which is the top output of encode conv</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>encoder_out_c</strong> – the value matrix for the attention-weighted sum,
which is the combination of base emb and top output of encode</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="onmt.modules.ConvMultiStepAttention.seq_linear">
<code class="descclassname">onmt.modules.ConvMultiStepAttention.</code><code class="descname">seq_linear</code><span class="sig-paren">(</span><em>linear</em>, <em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/ConvMultiStepAttention.html#seq_linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.ConvMultiStepAttention.seq_linear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-onmt.modules.CopyGenerator">
<span id="onmt-modules-copygenerator-module"></span><h2>onmt.modules.CopyGenerator module<a class="headerlink" href="#module-onmt.modules.CopyGenerator" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.CopyGenerator.CopyGenerator">
<em class="property">class </em><code class="descclassname">onmt.modules.CopyGenerator.</code><code class="descname">CopyGenerator</code><span class="sig-paren">(</span><em>opt</em>, <em>src_dict</em>, <em>tgt_dict</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/CopyGenerator.html#CopyGenerator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.CopyGenerator.CopyGenerator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Generator module that additionally considers copying
words directly from the source.</p>
<dl class="method">
<dt id="onmt.modules.CopyGenerator.CopyGenerator.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>hidden</em>, <em>attn</em>, <em>src_map</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/CopyGenerator.html#CopyGenerator.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.CopyGenerator.CopyGenerator.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes p(w) = p(z=1) p_{copy}(w|z=0)  +  p(z=0) * p_{softmax}(w|z=0)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.CopyGenerator.CopyGeneratorCriterion">
<em class="property">class </em><code class="descclassname">onmt.modules.CopyGenerator.</code><code class="descname">CopyGeneratorCriterion</code><span class="sig-paren">(</span><em>vocab_size</em>, <em>force_copy</em>, <em>pad</em>, <em>eps=1e-20</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/CopyGenerator.html#CopyGeneratorCriterion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.CopyGenerator.CopyGeneratorCriterion" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
</dd></dl>

<dl class="class">
<dt id="onmt.modules.CopyGenerator.CopyGeneratorLossCompute">
<em class="property">class </em><code class="descclassname">onmt.modules.CopyGenerator.</code><code class="descname">CopyGeneratorLossCompute</code><span class="sig-paren">(</span><em>generator</em>, <em>tgt_vocab</em>, <em>dataset</em>, <em>force_copy</em>, <em>eps=1e-20</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/CopyGenerator.html#CopyGeneratorLossCompute"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.CopyGenerator.CopyGeneratorLossCompute" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="onmt.html#onmt.Loss.LossComputeBase" title="onmt.Loss.LossComputeBase"><code class="xref py py-class docutils literal"><span class="pre">onmt.Loss.LossComputeBase</span></code></a></p>
<p>Copy Generator Loss Computation.</p>
<dl class="method">
<dt id="onmt.modules.CopyGenerator.CopyGeneratorLossCompute.compute_loss">
<code class="descname">compute_loss</code><span class="sig-paren">(</span><em>batch</em>, <em>output</em>, <em>target</em>, <em>copy_attn</em>, <em>align</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/CopyGenerator.html#CopyGeneratorLossCompute.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.CopyGenerator.CopyGeneratorLossCompute.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the loss. The args must match self.make_shard_state().
:param batch: the current batch.
:param output: the predict output from the model.
:param target: the validate target to compare output with.
:param copy_attn: the copy attention value.
:param align: the align info.</p>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.CopyGenerator.CopyGeneratorLossCompute.make_shard_state">
<code class="descname">make_shard_state</code><span class="sig-paren">(</span><em>batch</em>, <em>output</em>, <em>range_</em>, <em>attns</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/CopyGenerator.html#CopyGeneratorLossCompute.make_shard_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.CopyGenerator.CopyGeneratorLossCompute.make_shard_state" title="Permalink to this definition">¶</a></dt>
<dd><p>See base class for args description.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.Embeddings">
<span id="onmt-modules-embeddings-module"></span><h2>onmt.modules.Embeddings module<a class="headerlink" href="#module-onmt.modules.Embeddings" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.Embeddings.Embeddings">
<em class="property">class </em><code class="descclassname">onmt.modules.Embeddings.</code><code class="descname">Embeddings</code><span class="sig-paren">(</span><em>word_vec_size</em>, <em>position_encoding</em>, <em>feat_merge</em>, <em>feat_vec_exponent</em>, <em>feat_vec_size</em>, <em>dropout</em>, <em>word_padding_idx</em>, <em>feat_padding_idx</em>, <em>word_vocab_size</em>, <em>feat_vocab_sizes=[]</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Embeddings.html#Embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Embeddings.Embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Words embeddings dictionary for encoder/decoder.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>word_vec_size</strong> (<em>int</em>) – size of the dictionary of embeddings.</li>
<li><strong>position_encoding</strong> (<em>bool</em>) – use a sin to mark relative words positions.</li>
<li><strong>feat_merge</strong> (<em>string</em>) – merge action for the features embeddings:
concat, sum or mlp.</li>
<li><strong>feat_vec_exponent</strong> (<em>float</em>) – when using ‘-feat_merge concat’, feature
embedding size is N^feat_dim_exponent, where N is the
number of values of feature takes.</li>
<li><strong>feat_vec_size</strong> (<em>int</em>) – embedding dimension for features when using
‘-feat_merge mlp’</li>
<li><strong>dropout</strong> (<em>float</em>) – dropout probability.</li>
<li><strong>word_padding_idx</strong> (<em>int</em>) – padding index for words in the embeddings.</li>
<li><strong>feats_padding_idx</strong> (<em>[</em><em>int</em><em>]</em>) – padding index for a list of features
in the embeddings.</li>
<li><strong>word_vocab_size</strong> (<em>int</em>) – size of dictionary of embeddings for words.</li>
<li><strong>feat_vocab_sizes</strong> (<em>[</em><em>int</em><em>]</em><em>, </em><em>optional</em>) – list of size of dictionary
of embeddings for each feature.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="onmt.modules.Embeddings.Embeddings.emb_luts">
<code class="descname">emb_luts</code><a class="headerlink" href="#onmt.modules.Embeddings.Embeddings.emb_luts" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.Embeddings.Embeddings.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Embeddings.html#Embeddings.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Embeddings.Embeddings.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the embeddings for words, and features if there are any.
:param input: len x batch x nfeat
:type input: LongTensor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">len x batch x self.embedding_size</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">emb (FloatTensor)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.Embeddings.Embeddings.load_pretrained_vectors">
<code class="descname">load_pretrained_vectors</code><span class="sig-paren">(</span><em>emb_file</em>, <em>fixed</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Embeddings.html#Embeddings.load_pretrained_vectors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Embeddings.Embeddings.load_pretrained_vectors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="onmt.modules.Embeddings.Embeddings.word_lut">
<code class="descname">word_lut</code><a class="headerlink" href="#onmt.modules.Embeddings.Embeddings.word_lut" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Embeddings.PositionalEncoding">
<em class="property">class </em><code class="descclassname">onmt.modules.Embeddings.</code><code class="descname">PositionalEncoding</code><span class="sig-paren">(</span><em>dropout</em>, <em>dim</em>, <em>max_len=5000</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Embeddings.html#PositionalEncoding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Embeddings.PositionalEncoding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.Embeddings.PositionalEncoding.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>emb</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Embeddings.html#PositionalEncoding.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Embeddings.PositionalEncoding.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.Gate">
<span id="onmt-modules-gate-module"></span><h2>onmt.modules.Gate module<a class="headerlink" href="#module-onmt.modules.Gate" title="Permalink to this headline">¶</a></h2>
<p>Context gate is a decoder module that takes as input the previous word
embedding, the current decoder state and the attention state, and produces a
gate.
The gate can be used to select the input from the target side context
(decoder state), from the source context (attention state) or both.</p>
<dl class="class">
<dt id="onmt.modules.Gate.BothContextGate">
<em class="property">class </em><code class="descclassname">onmt.modules.Gate.</code><code class="descname">BothContextGate</code><span class="sig-paren">(</span><em>embeddings_size</em>, <em>decoder_size</em>, <em>attention_size</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Gate.html#BothContextGate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Gate.BothContextGate" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Apply the context gate to both contexts</p>
<dl class="method">
<dt id="onmt.modules.Gate.BothContextGate.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>prev_emb</em>, <em>dec_state</em>, <em>attn_state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Gate.html#BothContextGate.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Gate.BothContextGate.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Gate.ContextGate">
<em class="property">class </em><code class="descclassname">onmt.modules.Gate.</code><code class="descname">ContextGate</code><span class="sig-paren">(</span><em>embeddings_size</em>, <em>decoder_size</em>, <em>attention_size</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Gate.html#ContextGate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Gate.ContextGate" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implement up to the computation of the gate</p>
<dl class="method">
<dt id="onmt.modules.Gate.ContextGate.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>prev_emb</em>, <em>dec_state</em>, <em>attn_state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Gate.html#ContextGate.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Gate.ContextGate.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="onmt.modules.Gate.ContextGateFactory">
<code class="descclassname">onmt.modules.Gate.</code><code class="descname">ContextGateFactory</code><span class="sig-paren">(</span><em>type</em>, <em>embeddings_size</em>, <em>decoder_size</em>, <em>attention_size</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Gate.html#ContextGateFactory"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Gate.ContextGateFactory" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the correct ContextGate class</p>
</dd></dl>

<dl class="class">
<dt id="onmt.modules.Gate.SourceContextGate">
<em class="property">class </em><code class="descclassname">onmt.modules.Gate.</code><code class="descname">SourceContextGate</code><span class="sig-paren">(</span><em>embeddings_size</em>, <em>decoder_size</em>, <em>attention_size</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Gate.html#SourceContextGate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Gate.SourceContextGate" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Apply the context gate only to the source context</p>
<dl class="method">
<dt id="onmt.modules.Gate.SourceContextGate.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>prev_emb</em>, <em>dec_state</em>, <em>attn_state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Gate.html#SourceContextGate.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Gate.SourceContextGate.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Gate.TargetContextGate">
<em class="property">class </em><code class="descclassname">onmt.modules.Gate.</code><code class="descname">TargetContextGate</code><span class="sig-paren">(</span><em>embeddings_size</em>, <em>decoder_size</em>, <em>attention_size</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Gate.html#TargetContextGate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Gate.TargetContextGate" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Apply the context gate only to the target context</p>
<dl class="method">
<dt id="onmt.modules.Gate.TargetContextGate.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>prev_emb</em>, <em>dec_state</em>, <em>attn_state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Gate.html#TargetContextGate.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Gate.TargetContextGate.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.GlobalAttention">
<span id="onmt-modules-globalattention-module"></span><h2>onmt.modules.GlobalAttention module<a class="headerlink" href="#module-onmt.modules.GlobalAttention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.GlobalAttention.GlobalAttention">
<em class="property">class </em><code class="descclassname">onmt.modules.GlobalAttention.</code><code class="descname">GlobalAttention</code><span class="sig-paren">(</span><em>dim</em>, <em>coverage=False</em>, <em>attn_type='dot'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/GlobalAttention.html#GlobalAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.GlobalAttention.GlobalAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Luong Attention.</p>
<p>Global attention takes a matrix and a query vector. It
then computes a parameterized convex combination of the matrix
based on the input query.</p>
<blockquote>
<div><dl class="docutils">
<dt>H_1 H_2 H_3 … H_n</dt>
<dd><dl class="first last docutils">
<dt>q   q   q       q</dt>
<dd><div class="first last line-block">
<div class="line">|   |       |
|   |      /
        …..
      |  /
        a</div>
</div>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>Constructs a unit mapping.
$$(H_1 + H_n, q) =&gt; (a)$$
Where H is of <cite>batch x n x dim</cite> and q is of <cite>batch x dim</cite>.</p>
<p>Luong Attention (dot, general):
The full function is
$$  anh(W_2 [(softmax((W_1 q + b_1) H) H), q] + b_2)$$.</p>
<ul class="simple">
<li>dot: $$score(h_t,{overline{h}}_s) = h_t^T{overline{h}}_s$$</li>
<li>general: $$score(h_t,{overline{h}}_s) = h_t^T W_a {overline{h}}_s$$</li>
</ul>
<p>Bahdanau Attention (mlp):
$$c = sum_{j=1}^{SeqLength}_jh_j$$.
The Alignment-function $$a$$ computes an alignment as:
$$a_j = softmax(v_a^T       anh(W_a q + U_a h_j) )$$.</p>
<dl class="method">
<dt id="onmt.modules.GlobalAttention.GlobalAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>context</em>, <em>context_lengths=None</em>, <em>coverage=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/GlobalAttention.html#GlobalAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.GlobalAttention.GlobalAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>input (FloatTensor): batch x tgt_len x dim: decoder’s rnn’s output.
context (FloatTensor): batch x src_len x dim: src hidden states
context_lengths (LongTensor): the source context lengths.
coverage (FloatTensor): None (not supported yet)</p>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.GlobalAttention.GlobalAttention.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>h_t</em>, <em>h_s</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/GlobalAttention.html#GlobalAttention.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.GlobalAttention.GlobalAttention.score" title="Permalink to this definition">¶</a></dt>
<dd><p>h_t (FloatTensor): batch x tgt_len x dim
h_s (FloatTensor): batch x src_len x dim
returns scores (FloatTensor): batch x tgt_len x src_len:</p>
<blockquote>
<div>raw attention scores for each src index</div></blockquote>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.ImageEncoder">
<span id="onmt-modules-imageencoder-module"></span><h2>onmt.modules.ImageEncoder module<a class="headerlink" href="#module-onmt.modules.ImageEncoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.ImageEncoder.ImageEncoder">
<em class="property">class </em><code class="descclassname">onmt.modules.ImageEncoder.</code><code class="descname">ImageEncoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>bidirectional</em>, <em>rnn_size</em>, <em>dropout</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/ImageEncoder.html#ImageEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.ImageEncoder.ImageEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Encoder recurrent neural network for Images.</p>
<dl class="method">
<dt id="onmt.modules.ImageEncoder.ImageEncoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/ImageEncoder.html#ImageEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.ImageEncoder.ImageEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.ImageEncoder.ImageEncoder.load_pretrained_vectors">
<code class="descname">load_pretrained_vectors</code><span class="sig-paren">(</span><em>opt</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/ImageEncoder.html#ImageEncoder.load_pretrained_vectors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.ImageEncoder.ImageEncoder.load_pretrained_vectors" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.MultiHeadedAttn">
<span id="onmt-modules-multiheadedattn-module"></span><h2>onmt.modules.MultiHeadedAttn module<a class="headerlink" href="#module-onmt.modules.MultiHeadedAttn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.MultiHeadedAttn.MultiHeadedAttention">
<em class="property">class </em><code class="descclassname">onmt.modules.MultiHeadedAttn.</code><code class="descname">MultiHeadedAttention</code><span class="sig-paren">(</span><em>head_count</em>, <em>model_dim</em>, <em>p=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/MultiHeadedAttn.html#MultiHeadedAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.MultiHeadedAttn.MultiHeadedAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Multi-Head Attention module from
“Attention is All You Need”.</p>
<dl class="method">
<dt id="onmt.modules.MultiHeadedAttn.MultiHeadedAttention.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>key</em>, <em>value</em>, <em>query</em>, <em>mask=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/MultiHeadedAttn.html#MultiHeadedAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.MultiHeadedAttn.MultiHeadedAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.SRU">
<span id="onmt-modules-sru-module"></span><h2>onmt.modules.SRU module<a class="headerlink" href="#module-onmt.modules.SRU" title="Permalink to this headline">¶</a></h2>
<p>Implementation of “Training RNNs as Fast as CNNs”.
TODO: turn to pytorch’s implementation when it is available.</p>
<p>This implementation is adpoted from the author of the paper:
<a class="reference external" href="https://github.com/taolei87/sru/blob/master/cuda_functional.py">https://github.com/taolei87/sru/blob/master/cuda_functional.py</a>.</p>
<dl class="class">
<dt id="onmt.modules.SRU.CheckSRU">
<em class="property">class </em><code class="descclassname">onmt.modules.SRU.</code><code class="descname">CheckSRU</code><span class="sig-paren">(</span><em>option_strings</em>, <em>dest</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#CheckSRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.CheckSRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">argparse.Action</span></code></p>
</dd></dl>

<dl class="class">
<dt id="onmt.modules.SRU.SRU">
<em class="property">class </em><code class="descclassname">onmt.modules.SRU.</code><code class="descname">SRU</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>num_layers=2</em>, <em>dropout=0</em>, <em>rnn_dropout=0</em>, <em>bidirectional=False</em>, <em>use_tanh=1</em>, <em>use_relu=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.SRU.SRU.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>c0=None</em>, <em>return_hidden=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRU.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRU.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.SRU.SRU.set_bias">
<code class="descname">set_bias</code><span class="sig-paren">(</span><em>bias_val=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRU.set_bias"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRU.set_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.SRU.SRUCell">
<em class="property">class </em><code class="descclassname">onmt.modules.SRU.</code><code class="descname">SRUCell</code><span class="sig-paren">(</span><em>n_in</em>, <em>n_out</em>, <em>dropout=0</em>, <em>rnn_dropout=0</em>, <em>bidirectional=False</em>, <em>use_tanh=1</em>, <em>use_relu=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.SRU.SRUCell.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>c0=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRUCell.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRUCell.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.SRU.SRUCell.get_dropout_mask_">
<code class="descname">get_dropout_mask_</code><span class="sig-paren">(</span><em>size</em>, <em>p</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRUCell.get_dropout_mask_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRUCell.get_dropout_mask_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.SRU.SRUCell.init_weight">
<code class="descname">init_weight</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRUCell.init_weight"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRUCell.init_weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.SRU.SRUCell.set_bias">
<code class="descname">set_bias</code><span class="sig-paren">(</span><em>bias_val=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRUCell.set_bias"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRUCell.set_bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.SRU.SRU_Compute">
<em class="property">class </em><code class="descclassname">onmt.modules.SRU.</code><code class="descname">SRU_Compute</code><span class="sig-paren">(</span><em>activation_type</em>, <em>d_out</em>, <em>bidirectional=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRU_Compute"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRU_Compute" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.autograd.function.Function</span></code></p>
<dl class="method">
<dt id="onmt.modules.SRU.SRU_Compute.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>grad_h</em>, <em>grad_last</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRU_Compute.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRU_Compute.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.SRU.SRU_Compute.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>u</em>, <em>x</em>, <em>bias</em>, <em>init=None</em>, <em>mask_h=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#SRU_Compute.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.SRU_Compute.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="onmt.modules.SRU.check_sru_requirement">
<code class="descclassname">onmt.modules.SRU.</code><code class="descname">check_sru_requirement</code><span class="sig-paren">(</span><em>abort=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/SRU.html#check_sru_requirement"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.SRU.check_sru_requirement" title="Permalink to this definition">¶</a></dt>
<dd><p>Return True if check pass; if check fails and abort is True,
raise an Exception, othereise return False.</p>
</dd></dl>

</div>
<div class="section" id="module-onmt.modules.StackedRNN">
<span id="onmt-modules-stackedrnn-module"></span><h2>onmt.modules.StackedRNN module<a class="headerlink" href="#module-onmt.modules.StackedRNN" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.StackedRNN.StackedGRU">
<em class="property">class </em><code class="descclassname">onmt.modules.StackedRNN.</code><code class="descname">StackedGRU</code><span class="sig-paren">(</span><em>num_layers</em>, <em>input_size</em>, <em>rnn_size</em>, <em>dropout</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/StackedRNN.html#StackedGRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.StackedRNN.StackedGRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.StackedRNN.StackedGRU.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>hidden</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/StackedRNN.html#StackedGRU.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.StackedRNN.StackedGRU.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.StackedRNN.StackedLSTM">
<em class="property">class </em><code class="descclassname">onmt.modules.StackedRNN.</code><code class="descname">StackedLSTM</code><span class="sig-paren">(</span><em>num_layers</em>, <em>input_size</em>, <em>rnn_size</em>, <em>dropout</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/StackedRNN.html#StackedLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.StackedRNN.StackedLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Our own implementation of stacked LSTM.
Needed for the decoder, because we do input feeding.</p>
<dl class="method">
<dt id="onmt.modules.StackedRNN.StackedLSTM.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>hidden</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/StackedRNN.html#StackedLSTM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.StackedRNN.StackedLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.StructuredAttention">
<span id="onmt-modules-structuredattention-module"></span><h2>onmt.modules.StructuredAttention module<a class="headerlink" href="#module-onmt.modules.StructuredAttention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.StructuredAttention.MatrixTree">
<em class="property">class </em><code class="descclassname">onmt.modules.StructuredAttention.</code><code class="descname">MatrixTree</code><span class="sig-paren">(</span><em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/StructuredAttention.html#MatrixTree"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.StructuredAttention.MatrixTree" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implementation of the matrix-tree theorem for computing marginals
of non-projective dependency parsing. This attention layer is used
in the paper “Learning Structured Text Representations.”</p>
<p>qq</p>
<dl class="method">
<dt id="onmt.modules.StructuredAttention.MatrixTree.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/StructuredAttention.html#MatrixTree.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.StructuredAttention.MatrixTree.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.Transformer">
<span id="onmt-modules-transformer-module"></span><h2>onmt.modules.Transformer module<a class="headerlink" href="#module-onmt.modules.Transformer" title="Permalink to this headline">¶</a></h2>
<p>Implementation of “Attention is All You Need”</p>
<dl class="class">
<dt id="onmt.modules.Transformer.PositionwiseFeedForward">
<em class="property">class </em><code class="descclassname">onmt.modules.Transformer.</code><code class="descname">PositionwiseFeedForward</code><span class="sig-paren">(</span><em>size</em>, <em>hidden_size</em>, <em>dropout=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#PositionwiseFeedForward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.PositionwiseFeedForward" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A two-layer Feed-Forward-Network.</p>
<dl class="method">
<dt id="onmt.modules.Transformer.PositionwiseFeedForward.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#PositionwiseFeedForward.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.PositionwiseFeedForward.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Transformer.TransformerDecoder">
<em class="property">class </em><code class="descclassname">onmt.modules.Transformer.</code><code class="descname">TransformerDecoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>hidden_size</em>, <em>attn_type</em>, <em>copy_attn</em>, <em>dropout</em>, <em>embeddings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The Transformer decoder from “Attention is All You Need”.</p>
<dl class="method">
<dt id="onmt.modules.Transformer.TransformerDecoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>context</em>, <em>state</em>, <em>context_lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward through the TransformerDecoder.
:param input: a sequence of input tokens tensors</p>
<blockquote>
<div>of size (len x batch x nfeats).</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>context</strong> (<em>FloatTensor</em>) – output(tensor sequence) from the encoder
of size (src_len x batch x hidden_size).</li>
<li><strong>state</strong> (<em>FloatTensor</em>) – hidden state from the encoder RNN for
initializing the decoder.</li>
<li><strong>context_lengths</strong> (<em>LongTensor</em>) – the source context lengths, this is
not used for TransformerDecoder, but
for interface compatibility.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>a Tensor sequence of output from the decoder</dt>
<dd><p class="first last">of shape (len x batch x hidden_size).</p>
</dd>
</dl>
<p>state (FloatTensor): final hidden state from the decoder.
attns (dict of (str, FloatTensor)): a dictionary of different</p>
<blockquote>
<div><p>type of attention Tensor from the decoder
of shape (src_len x batch).</p>
</div></blockquote>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">outputs (FloatTensor)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.Transformer.TransformerDecoder.init_decoder_state">
<code class="descname">init_decoder_state</code><span class="sig-paren">(</span><em>src</em>, <em>context</em>, <em>enc_hidden</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerDecoder.init_decoder_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerDecoder.init_decoder_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Transformer.TransformerDecoderLayer">
<em class="property">class </em><code class="descclassname">onmt.modules.Transformer.</code><code class="descname">TransformerDecoderLayer</code><span class="sig-paren">(</span><em>size</em>, <em>dropout</em>, <em>head_count=8</em>, <em>hidden_size=2048</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerDecoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerDecoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.Transformer.TransformerDecoderLayer.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>context</em>, <em>src_pad_mask</em>, <em>tgt_pad_mask</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerDecoderLayer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerDecoderLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Transformer.TransformerDecoderState">
<em class="property">class </em><code class="descclassname">onmt.modules.Transformer.</code><code class="descname">TransformerDecoderState</code><span class="sig-paren">(</span><em>src</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerDecoderState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerDecoderState" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="onmt.html#onmt.Models.DecoderState" title="onmt.Models.DecoderState"><code class="xref py py-class docutils literal"><span class="pre">onmt.Models.DecoderState</span></code></a></p>
<dl class="method">
<dt id="onmt.modules.Transformer.TransformerDecoderState.repeat_beam_size_times">
<code class="descname">repeat_beam_size_times</code><span class="sig-paren">(</span><em>beam_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerDecoderState.repeat_beam_size_times"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerDecoderState.repeat_beam_size_times" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeat beam_size times along batch dimension.</p>
</dd></dl>

<dl class="method">
<dt id="onmt.modules.Transformer.TransformerDecoderState.update_state">
<code class="descname">update_state</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerDecoderState.update_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerDecoderState.update_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Called for every decoder forward pass.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Transformer.TransformerEncoder">
<em class="property">class </em><code class="descclassname">onmt.modules.Transformer.</code><code class="descname">TransformerEncoder</code><span class="sig-paren">(</span><em>num_layers</em>, <em>hidden_size</em>, <em>dropout</em>, <em>embeddings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="onmt.html#onmt.Models.EncoderBase" title="onmt.Models.EncoderBase"><code class="xref py py-class docutils literal"><span class="pre">onmt.Models.EncoderBase</span></code></a></p>
<p>The Transformer encoder from “Attention is All You Need”.</p>
<dl class="method">
<dt id="onmt.modules.Transformer.TransformerEncoder.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>lengths=None</em>, <em>hidden=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>See EncoderBase.forward() for description of args and returns.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.Transformer.TransformerEncoderLayer">
<em class="property">class </em><code class="descclassname">onmt.modules.Transformer.</code><code class="descname">TransformerEncoderLayer</code><span class="sig-paren">(</span><em>size</em>, <em>dropout</em>, <em>head_count=8</em>, <em>hidden_size=2048</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerEncoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerEncoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.Transformer.TransformerEncoderLayer.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em>, <em>mask</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/Transformer.html#TransformerEncoderLayer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.Transformer.TransformerEncoderLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.UtilClass">
<span id="onmt-modules-utilclass-module"></span><h2>onmt.modules.UtilClass module<a class="headerlink" href="#module-onmt.modules.UtilClass" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.modules.UtilClass.Bottle">
<em class="property">class </em><code class="descclassname">onmt.modules.UtilClass.</code><code class="descname">Bottle</code><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#Bottle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.Bottle" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.UtilClass.Bottle.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#Bottle.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.Bottle.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.UtilClass.Bottle2">
<em class="property">class </em><code class="descclassname">onmt.modules.UtilClass.</code><code class="descname">Bottle2</code><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#Bottle2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.Bottle2" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="onmt.modules.UtilClass.Bottle2.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#Bottle2.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.Bottle2.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.UtilClass.BottleLayerNorm">
<em class="property">class </em><code class="descclassname">onmt.modules.UtilClass.</code><code class="descname">BottleLayerNorm</code><span class="sig-paren">(</span><em>d_hid</em>, <em>eps=0.001</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#BottleLayerNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.BottleLayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#onmt.modules.UtilClass.Bottle" title="onmt.modules.UtilClass.Bottle"><code class="xref py py-class docutils literal"><span class="pre">onmt.modules.UtilClass.Bottle</span></code></a>, <a class="reference internal" href="#onmt.modules.UtilClass.LayerNorm" title="onmt.modules.UtilClass.LayerNorm"><code class="xref py py-class docutils literal"><span class="pre">onmt.modules.UtilClass.LayerNorm</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="onmt.modules.UtilClass.BottleLinear">
<em class="property">class </em><code class="descclassname">onmt.modules.UtilClass.</code><code class="descname">BottleLinear</code><span class="sig-paren">(</span><em>in_features</em>, <em>out_features</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#BottleLinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.BottleLinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#onmt.modules.UtilClass.Bottle" title="onmt.modules.UtilClass.Bottle"><code class="xref py py-class docutils literal"><span class="pre">onmt.modules.UtilClass.Bottle</span></code></a>, <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.linear.Linear</span></code></p>
</dd></dl>

<dl class="class">
<dt id="onmt.modules.UtilClass.BottleSoftmax">
<em class="property">class </em><code class="descclassname">onmt.modules.UtilClass.</code><code class="descname">BottleSoftmax</code><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#BottleSoftmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.BottleSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#onmt.modules.UtilClass.Bottle" title="onmt.modules.UtilClass.Bottle"><code class="xref py py-class docutils literal"><span class="pre">onmt.modules.UtilClass.Bottle</span></code></a>, <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.activation.Softmax</span></code></p>
</dd></dl>

<dl class="class">
<dt id="onmt.modules.UtilClass.Elementwise">
<em class="property">class </em><code class="descclassname">onmt.modules.UtilClass.</code><code class="descname">Elementwise</code><span class="sig-paren">(</span><em>merge=None</em>, <em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#Elementwise"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.Elementwise" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.container.ModuleList</span></code></p>
<p>A simple network container.
Parameters are a list of modules.
Inputs are a 3d Variable whose last dimension is the same length
as the list.
Outputs are the result of applying modules to inputs elementwise.
An optional merge parameter allows the outputs to be reduced to a
single Variable.</p>
<dl class="method">
<dt id="onmt.modules.UtilClass.Elementwise.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#Elementwise.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.Elementwise.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.UtilClass.LayerNorm">
<em class="property">class </em><code class="descclassname">onmt.modules.UtilClass.</code><code class="descname">LayerNorm</code><span class="sig-paren">(</span><em>d_hid</em>, <em>eps=0.001</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#LayerNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.LayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Layer normalization module</p>
<dl class="method">
<dt id="onmt.modules.UtilClass.LayerNorm.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>z</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/UtilClass.html#LayerNorm.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.UtilClass.LayerNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-onmt.modules.WeightNorm">
<span id="onmt-modules-weightnorm-module"></span><h2>onmt.modules.WeightNorm module<a class="headerlink" href="#module-onmt.modules.WeightNorm" title="Permalink to this headline">¶</a></h2>
<p>Implementation of “Weight Normalization: A Simple Reparameterization
to Accelerate Training of Deep Neural Networks”
As a reparameterization method, weight normalization is same
as BatchNormalization, but it doesn’t depend on minibatch.</p>
<dl class="class">
<dt id="onmt.modules.WeightNorm.WeightNormConv2d">
<em class="property">class </em><code class="descclassname">onmt.modules.WeightNorm.</code><code class="descname">WeightNormConv2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>init_scale=1.0</em>, <em>polyak_decay=0.9995</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#WeightNormConv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.conv.Conv2d</span></code></p>
<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormConv2d.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>init=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#WeightNormConv2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConv2d.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormConv2d.reset_parameters">
<code class="descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#WeightNormConv2d.reset_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConv2d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.WeightNorm.WeightNormConvTranspose2d">
<em class="property">class </em><code class="descclassname">onmt.modules.WeightNorm.</code><code class="descname">WeightNormConvTranspose2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>init_scale=1.0</em>, <em>polyak_decay=0.9995</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#WeightNormConvTranspose2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.conv.ConvTranspose2d</span></code></p>
<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormConvTranspose2d.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>init=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#WeightNormConvTranspose2d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConvTranspose2d.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormConvTranspose2d.reset_parameters">
<code class="descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#WeightNormConvTranspose2d.reset_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormConvTranspose2d.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.modules.WeightNorm.WeightNormLinear">
<em class="property">class </em><code class="descclassname">onmt.modules.WeightNorm.</code><code class="descname">WeightNormLinear</code><span class="sig-paren">(</span><em>in_features</em>, <em>out_features</em>, <em>init_scale=1.0</em>, <em>polyak_decay=0.9995</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#WeightNormLinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormLinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">torch.nn.modules.linear.Linear</span></code></p>
<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormLinear.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em>, <em>init=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#WeightNormLinear.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormLinear.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="onmt.modules.WeightNorm.WeightNormLinear.reset_parameters">
<code class="descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#WeightNormLinear.reset_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.WeightNormLinear.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="onmt.modules.WeightNorm.get_var_maybe_avg">
<code class="descclassname">onmt.modules.WeightNorm.</code><code class="descname">get_var_maybe_avg</code><span class="sig-paren">(</span><em>namespace</em>, <em>var_name</em>, <em>training</em>, <em>polyak_decay</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#get_var_maybe_avg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.get_var_maybe_avg" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="onmt.modules.WeightNorm.get_vars_maybe_avg">
<code class="descclassname">onmt.modules.WeightNorm.</code><code class="descname">get_vars_maybe_avg</code><span class="sig-paren">(</span><em>namespace</em>, <em>var_names</em>, <em>training</em>, <em>polyak_decay</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/modules/WeightNorm.html#get_vars_maybe_avg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.modules.WeightNorm.get_vars_maybe_avg" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-contents">
<h2>Module contents<a class="headerlink" href="#module-contents" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-onmt.modules"></span></div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="CONTRIBUTORS.html" class="btn btn-neutral float-right" title="Contributors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="onmt.html" class="btn btn-neutral" title="onmt package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, srush.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>