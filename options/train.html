

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Options: train.py: &mdash; OpenNMT-py  documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="OpenNMT-py  documentation" href="../index.html"/>
        <link rel="next" title="Options: translate.py:" href="translate.html"/>
        <link rel="prev" title="Options: preprocess.py:" href="preprocess.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> OpenNMT-py
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../main.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extended.html">Example: Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Summarization.html">Example: Summarization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../im2text.html">Example: Image to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech2text.html">Example: Speech to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocess.html">Options: preprocess.py:</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Options: train.py:</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-embeddings"><strong>Model-Embeddings</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-embedding-features"><strong>Model-Embedding Features</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-encoder-decoder"><strong>Model- Encoder-Decoder</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-attention"><strong>Model- Attention</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#general"><strong>General</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#initialization"><strong>Initialization</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimization-type"><strong>Optimization- Type</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimization-rate"><strong>Optimization- Rate</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#logging"><strong>Logging</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#speech"><strong>Speech</strong>:</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="translate.html">Options: translate.py:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onmt.html">onmt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onmt.modules.html">onmt.modules package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CONTRIBUTORS.html">Contributors</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OpenNMT-py</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Options: train.py:</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/options/train.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <!--- This file was automatically generated. Do not modify it manually but use the docs/options/generate.sh script instead. --><p>train.py</p>
<div class="section" id="options-train-py">
<span id="options-train-py"></span><h1>Options: train.py:<a class="headerlink" href="#options-train-py" title="Permalink to this headline">¶</a></h1>
<p>train.py</p>
<div class="section" id="model-embeddings">
<span id="model-embeddings"></span><h2><strong>Model-Embeddings</strong>:<a class="headerlink" href="#model-embeddings" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>-src_word_vec_size [500]</strong>
Word embedding size for src.</li>
<li><strong>-tgt_word_vec_size [500]</strong>
Word embedding size for tgt.</li>
<li><strong>-word_vec_size [-1]</strong>
Word embedding size for src and tgt.</li>
<li><strong>-share_decoder_embeddings []</strong>
Use a shared weight matrix for the input and output word embeddings in the
decoder.</li>
<li><strong>-share_embeddings []</strong>
Share the word embeddings between encoder and decoder. Need to use shared
dictionary for this option.</li>
<li><strong>-position_encoding []</strong>
Use a sin to mark relative words positions. Necessary for non-RNN style models.</li>
</ul>
</div>
<div class="section" id="model-embedding-features">
<span id="model-embedding-features"></span><h2><strong>Model-Embedding Features</strong>:<a class="headerlink" href="#model-embedding-features" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>-feat_merge [concat]</strong>
Merge action for incorporating features embeddings. Options [concat|sum|mlp].</li>
<li><strong>-feat_vec_size [-1]</strong>
If specified, feature embedding sizes will be set to this. Otherwise,
feat_vec_exponent will be used.</li>
<li><strong>-feat_vec_exponent [0.7]</strong>
If -feat_merge_size is not set, feature embedding sizes will be set to
N^feat_vec_exponent where N is the number of values the feature takes.</li>
</ul>
</div>
<div class="section" id="model-encoder-decoder">
<span id="model-encoder-decoder"></span><h2><strong>Model- Encoder-Decoder</strong>:<a class="headerlink" href="#model-encoder-decoder" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>-model_type [text]</strong>
Type of source model to use. Allows the system to incorporate non-text inputs.
Options are [text|img|audio].</li>
<li><strong>-encoder_type [rnn]</strong>
Type of encoder layer to use. Non-RNN layers are experimental. Options are
[rnn|brnn|mean|transformer|cnn].</li>
<li><strong>-decoder_type [rnn]</strong>
Type of decoder layer to use. Non-RNN layers are experimental. Options are
[rnn|transformer|cnn].</li>
<li><strong>-layers [-1]</strong>
Number of layers in enc/dec.</li>
<li><strong>-enc_layers [2]</strong>
Number of layers in the encoder</li>
<li><strong>-dec_layers [2]</strong>
Number of layers in the decoder</li>
<li><strong>-rnn_size [500]</strong>
Size of rnn hidden states</li>
<li><strong>-cnn_kernel_width [3]</strong>
Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in conv
layer</li>
<li><strong>-input_feed [1]</strong>
Feed the context vector at each time step as additional input (via concatenation
with the word embeddings) to the decoder.</li>
<li><strong>-rnn_type [LSTM]</strong>
The gate type to use in the RNNs</li>
<li><strong>-brnn []</strong>
Deprecated, use <code class="docutils literal"><span class="pre">encoder_type</span></code>.</li>
<li><strong>-brnn_merge [concat]</strong>
Merge action for the bidir hidden states</li>
<li><strong>-context_gate []</strong>
Type of context gate to use. Do not select for no context gate.</li>
</ul>
</div>
<div class="section" id="model-attention">
<span id="model-attention"></span><h2><strong>Model- Attention</strong>:<a class="headerlink" href="#model-attention" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>-global_attention [general]</strong>
The attention type to use: dotprod or general (Luong) or MLP (Bahdanau)</li>
<li><strong>-copy_attn []</strong>
Train copy attention layer.</li>
<li><strong>-copy_attn_force []</strong>
When available, train to copy.</li>
<li><strong>-coverage_attn []</strong>
Train a coverage attention layer.</li>
<li><strong>-lambda_coverage [1]</strong>
Lambda value for coverage.</li>
</ul>
</div>
<div class="section" id="general">
<span id="general"></span><h2><strong>General</strong>:<a class="headerlink" href="#general" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>-data []</strong>
Path prefix to the &quot;.train.pt&quot; and &quot;.valid.pt&quot; file path from preprocess.py</li>
<li><strong>-save_model [model]</strong>
Model filename (the model will be saved as &lt;save_model&gt;_epochN_PPL.pt where PPL
is the validation perplexity</li>
<li><strong>-gpuid []</strong>
Use CUDA on the listed devices.</li>
<li><strong>-seed [-1]</strong>
Random seed used for the experiments reproducibility.</li>
</ul>
</div>
<div class="section" id="initialization">
<span id="initialization"></span><h2><strong>Initialization</strong>:<a class="headerlink" href="#initialization" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>-start_epoch [1]</strong>
The epoch from which to start</li>
<li><strong>-param_init [0.1]</strong>
Parameters are initialized over uniform distribution with support (-param_init,
param_init). Use 0 to not use initialization</li>
<li><strong>-train_from []</strong>
If training from a checkpoint then this is the path to the pretrained model's
state_dict.</li>
<li><strong>-pre_word_vecs_enc []</strong>
If a valid path is specified, then this will load pretrained word embeddings on
the encoder side. See README for specific formatting instructions.</li>
<li><strong>-pre_word_vecs_dec []</strong>
If a valid path is specified, then this will load pretrained word embeddings on
the decoder side. See README for specific formatting instructions.</li>
<li><strong>-fix_word_vecs_enc []</strong>
Fix word embeddings on the encoder side.</li>
<li><strong>-fix_word_vecs_dec []</strong>
Fix word embeddings on the encoder side.</li>
</ul>
</div>
<div class="section" id="optimization-type">
<span id="optimization-type"></span><h2><strong>Optimization- Type</strong>:<a class="headerlink" href="#optimization-type" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>-batch_size [64]</strong>
Maximum batch size</li>
<li><strong>-max_generator_batches [32]</strong>
Maximum batches of words in a sequence to run the generator on in parallel.
Higher is faster, but uses more memory.</li>
<li><strong>-epochs [13]</strong>
Number of training epochs</li>
<li><strong>-optim [sgd]</strong>
Optimization method.</li>
<li><strong>-adagrad_accumulator_init []</strong>
Initializes the accumulator values in adagrad. Mirrors the
initial_accumulator_value option in the tensorflow adagrad (use 0.1 for their
default).</li>
<li><strong>-max_grad_norm [5]</strong>
If the norm of the gradient vector exceeds this, renormalize it to have the norm
equal to max_grad_norm</li>
<li><strong>-dropout [0.3]</strong>
Dropout probability; applied in LSTM stacks.</li>
<li><strong>-truncated_decoder []</strong>
Truncated bptt.</li>
<li><strong>-adam_beta1 [0.9]</strong>
The beta1 parameter used by Adam. Almost without exception a value of 0.9 is
used in the literature, seemingly giving good results, so we would discourage
changing this value from the default without due consideration.</li>
<li><strong>-adam_beta2 [0.999]</strong>
The beta2 parameter used by Adam. Typically a value of 0.999 is recommended, as
this is the value suggested by the original paper describing Adam, and is also
the value adopted in other frameworks such as Tensorflow and Kerras, i.e. see:
https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer
https://keras.io/optimizers/ . Whereas recently the paper &quot;Attention is All You
Need&quot; suggested a value of 0.98 for beta2, this parameter may not work well for
normal models / default baselines.</li>
</ul>
</div>
<div class="section" id="optimization-rate">
<span id="optimization-rate"></span><h2><strong>Optimization- Rate</strong>:<a class="headerlink" href="#optimization-rate" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>-learning_rate [1.0]</strong>
Starting learning rate. Recommended settings: sgd = 1, adagrad = 0.1, adadelta =
1, adam = 0.001</li>
<li><strong>-learning_rate_decay [0.5]</strong>
If update_learning_rate, decay learning rate by this much if (i) perplexity does
not decrease on the validation set or (ii) epoch has gone past start_decay_at</li>
<li><strong>-start_decay_at [8]</strong>
Start decaying every epoch after and including this epoch</li>
<li><strong>-start_checkpoint_at []</strong>
Start checkpointing every epoch after and including this epoch</li>
<li><strong>-decay_method []</strong>
Use a custom decay rate.</li>
<li><strong>-warmup_steps [4000]</strong>
Number of warmup steps for custom decay.</li>
</ul>
</div>
<div class="section" id="logging">
<span id="logging"></span><h2><strong>Logging</strong>:<a class="headerlink" href="#logging" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>-report_every [50]</strong>
Print stats at this interval.</li>
<li><strong>-exp_host []</strong>
Send logs to this crayon server.</li>
<li><strong>-exp []</strong>
Name of the experiment for logging.</li>
</ul>
</div>
<div class="section" id="speech">
<span id="speech"></span><h2><strong>Speech</strong>:<a class="headerlink" href="#speech" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>-sample_rate [16000]</strong>
Sample rate.</li>
<li><strong>-window_size [0.02]</strong>
Window size for spectrogram in seconds.</li>
</ul>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="translate.html" class="btn btn-neutral float-right" title="Options: translate.py:" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="preprocess.html" class="btn btn-neutral" title="Options: preprocess.py:" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, srush.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>